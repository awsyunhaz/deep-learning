{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Trains a GAN to generate synthetic images of MNIST digits.\n",
    "\n",
    "A Generative Adversarial Network (GAN) is a generative model that learns the\n",
    "probability distribution of training examples and generates similar samples. It\n",
    "simultaneously trains two models: a generative model G that learns the data\n",
    "distribution, and a discriminative model D that estimates the probability that a\n",
    "sample came from the training data [1].\n",
    "\n",
    "To learn the generator's distribution p_g over data z, we define a multilayer\n",
    "perceptron G(z; theta_g), where the prior of input noise z~p_z(z) is a Gaussian\n",
    "distribution. We also define a second multilayer perceptron D(x; theta_d)\n",
    "that outputs a single scalar. D(x) represents the probability that x came\n",
    "from the training data rather than p_g.\n",
    "\n",
    "We train D to maximize the probability of assigning the correct label to both\n",
    "training examples and samples from G. We simultaneously train G to maximize the\n",
    "probability of D making a mistake. This framework corresponds to a minimax\n",
    "two-player game with value function V(G, D):\n",
    "\n",
    "```none\n",
    "V(G, D) = E_{x~p_data(x)}[log(D(x))] + E_{z~p_z(z)}[log(1-D(G(z)))]\n",
    "```\n",
    "\n",
    "This optimization problem is bilevel: it requires a minima solution with respect\n",
    "to generative parameters theta_g and a maxima solution with respect to\n",
    "discriminative parameters theta_d. In practice, the algorithm proceeds by\n",
    "iterating gradient updates on each network. The goal of training is to reach\n",
    "the equilibrium where the generator produces samples that are indistinguishable\n",
    "by the discriminator.\n",
    "\n",
    "#### References\n",
    "\n",
    "[1]: Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu,\n",
    "     David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio.\n",
    "     Generative Adversarial Nets. In _Neural Information Processing\n",
    "     Systems Conference_, 2014.\n",
    "     https://arxiv.org/pdf/1406.2661.pdf\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "# Dependency imports\n",
    "from absl import flags\n",
    "from matplotlib.backends import backend_agg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets import mnist\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "IMAGE_SHAPE = [28, 28, 1]\n",
    "\n",
    "tf.flags.DEFINE_float('learning_rate',\n",
    "                      default=1e-4,\n",
    "                      help='Initial learning rate.')\n",
    "tf.flags.DEFINE_integer('max_steps',\n",
    "                        default=100000,\n",
    "                        help='Number of training steps to run.')\n",
    "tf.flags.DEFINE_integer('batch_size',\n",
    "                        default=128,\n",
    "                        help='Batch size.')\n",
    "tf.flags.DEFINE_integer('hidden_size',\n",
    "                        default=128,\n",
    "                        help='Hidden layer size.')\n",
    "flags.DEFINE_integer('viz_steps',\n",
    "                     default=1000,\n",
    "                     help='Frequency at which save generated images.')\n",
    "flags.DEFINE_string('data_dir',\n",
    "                    default='/tmp/data',\n",
    "                    help='Directory where data is stored (if using real data)')\n",
    "flags.DEFINE_string(\n",
    "    'model_dir',\n",
    "    default=os.path.join(os.getenv('TEST_TMPDIR', '/tmp'),\n",
    "                         'generative_adversarial_network/'),\n",
    "    help=\"Directory to put the model's fit.\")\n",
    "flags.DEFINE_bool('fake_data',\n",
    "                  default=None,\n",
    "                  help='If true, uses fake data. Defaults to real data.')\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_pipeline(train_images, batch_size):\n",
    "  \"\"\"Build an iterator over training batches.\"\"\"\n",
    "\n",
    "  training_dataset = tf.data.Dataset.from_tensor_slices(train_images)\n",
    "  training_batches = training_dataset.shuffle(\n",
    "      50000, reshuffle_each_iteration=True).repeat().batch(batch_size)\n",
    "  training_iterator = tf.compat.v1.data.make_one_shot_iterator(training_batches)\n",
    "  images = training_iterator.get_next()\n",
    "  return images\n",
    "\n",
    "\n",
    "def build_fake_data(size):\n",
    "  \"\"\"Generate fake images of MNIST digits.\"\"\"\n",
    "\n",
    "  # Generate random noise from a Gaussian distribution.\n",
    "  fake_images = np.float32(np.random.normal(size=size))\n",
    "  return fake_images\n",
    "\n",
    "\n",
    "def plot_generated_images(images, fname):\n",
    "  \"\"\"Save a synthetic image as a PNG file.\n",
    "\n",
    "  Args:\n",
    "    images: samples of synthetic images generated by the generative network.\n",
    "    fname: Python `str`, filename to save the plot to.\n",
    "  \"\"\"\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "  canvas = backend_agg.FigureCanvasAgg(fig)\n",
    "\n",
    "  for i, image in enumerate(images):\n",
    "    ax = fig.add_subplot(4, 4, i + 1)\n",
    "    plt.axis('off')\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.imshow(image.reshape(IMAGE_SHAPE[:-1]), cmap='Greys_r')\n",
    "\n",
    "  fig.tight_layout()\n",
    "  plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "  canvas.print_figure(fname, format='png')\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "  del argv  # unused\n",
    "  if tf.io.gfile.exists(FLAGS.model_dir):\n",
    "    tf.compat.v1.logging.warning(\n",
    "        'Warning: deleting old log directory at {}'.format(FLAGS.model_dir))\n",
    "    tf.io.gfile.rmtree(FLAGS.model_dir)\n",
    "  tf.io.gfile.makedirs(FLAGS.model_dir)\n",
    "\n",
    "  # Collapse the image data dimension for use with a fully-connected layer.\n",
    "  image_size = np.prod(IMAGE_SHAPE, dtype=np.int32)\n",
    "  if FLAGS.fake_data:\n",
    "    train_images = build_fake_data([10, image_size])\n",
    "  else:\n",
    "    mnist_data = mnist.read_data_sets(FLAGS.data_dir, reshape=image_size)\n",
    "    train_images = mnist_data.train.images\n",
    "\n",
    "  images = build_input_pipeline(train_images, FLAGS.batch_size)\n",
    "\n",
    "  # Build a Generative network. We use the Flipout Monte Carlo estimator\n",
    "  # for the fully-connected layers: this enables lower variance stochastic\n",
    "  # gradients than naive reparameterization.\n",
    "  with tf.compat.v1.name_scope('Generator'):\n",
    "    random_noise = tf.placeholder(tf.float64, shape=[None, FLAGS.hidden_size])\n",
    "    generative_net = tf.keras.Sequential([\n",
    "        tfp.layers.DenseFlipout(FLAGS.hidden_size, activation=tf.nn.relu),\n",
    "        tfp.layers.DenseFlipout(image_size, activation=tf.sigmoid)\n",
    "    ])\n",
    "    sythetic_image = generative_net(random_noise)\n",
    "\n",
    "  # Build a Discriminative network. Define the model as a Bernoulli\n",
    "  # distribution parameterized by logits from a fully-connected layer.\n",
    "  with tf.compat.v1.name_scope('Discriminator'):\n",
    "    discriminative_net = tf.keras.Sequential([\n",
    "        tfp.layers.DenseFlipout(FLAGS.hidden_size, activation=tf.nn.relu),\n",
    "        tfp.layers.DenseFlipout(1)\n",
    "    ])\n",
    "    logits_real = discriminative_net(images)\n",
    "    logits_fake = discriminative_net(sythetic_image)\n",
    "    labels_distribution_real = tfd.Bernoulli(logits=logits_real)\n",
    "    labels_distribution_fake = tfd.Bernoulli(logits=logits_fake)\n",
    "\n",
    "  # Compute the model loss for discrimator and generator, averaged over\n",
    "  # the batch size.\n",
    "  loss_real = -tf.reduce_mean(\n",
    "      input_tensor=labels_distribution_real.log_prob(\n",
    "          tf.ones_like(logits_real)))\n",
    "  loss_fake = -tf.reduce_mean(\n",
    "      input_tensor=labels_distribution_fake.log_prob(\n",
    "          tf.zeros_like(logits_fake)))\n",
    "  loss_discriminator = loss_real + loss_fake\n",
    "  loss_generator = -tf.reduce_mean(\n",
    "      input_tensor=labels_distribution_fake.log_prob(\n",
    "          tf.ones_like(logits_fake)))\n",
    "\n",
    "  with tf.compat.v1.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "    train_op_discriminator = optimizer.minimize(\n",
    "        loss_discriminator,\n",
    "        var_list=tf.get_collection(\n",
    "            tf.GraphKeys.TRAINABLE_VARIABLES, scope='Discriminator'))\n",
    "    train_op_generator = optimizer.minimize(\n",
    "        loss_generator,\n",
    "        var_list=tf.get_collection(\n",
    "            tf.GraphKeys.TRAINABLE_VARIABLES, scope='Generator'))\n",
    "\n",
    "  with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(FLAGS.max_steps + 1):\n",
    "      # Iterate gradient updates on each network.\n",
    "      _, loss_value_d = sess.run([train_op_discriminator, loss_discriminator],\n",
    "                                 feed_dict={random_noise: build_fake_data(\n",
    "                                     [FLAGS.batch_size, FLAGS.hidden_size])})\n",
    "      _, loss_value_g = sess.run([train_op_generator, loss_generator],\n",
    "                                 feed_dict={random_noise: build_fake_data(\n",
    "                                     [FLAGS.batch_size, FLAGS.hidden_size])})\n",
    "\n",
    "      # Visualize some sythetic images produced by the generative network.\n",
    "      if step % FLAGS.viz_steps == 0:\n",
    "        images = sess.run(sythetic_image,\n",
    "                          feed_dict={random_noise: build_fake_data(\n",
    "                              [16, FLAGS.hidden_size])})\n",
    "\n",
    "        plot_generated_images(images, fname=os.path.join(\n",
    "            FLAGS.model_dir,\n",
    "            'step{:06d}_images.png'.format(step)))\n",
    "\n",
    "        print('Step: {:>3d} Loss_discriminator: {:.3f} '\n",
    "              'Loss_generator: {:.3f}'.format(step, loss_value_d, loss_value_g))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "  tf.compat.v1.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
