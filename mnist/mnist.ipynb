{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "INPUT_SIZE = 784\n",
    "OUTPUT_SIZE = 10\n",
    "HIDDEN_SIZE = 512\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 10000\n",
    "MOVING_AVERAGE_DECAY = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def inference(input_tensor, avg_class, w1, b1, w2, b2):\n",
    "#     if avg_class == None:\n",
    "#         layer1 = tf.nn.relu(tf.matmul(input_tensor, w1) + b1)\n",
    "#         # softmax is in loss function\n",
    "#         layer2 = tf.matmul(layer1, w2) + b2\n",
    "#         return layer2\n",
    "#     else:\n",
    "#         layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(w1)) + avg_class.average(b1))\n",
    "#         layer2 = tf.matmul(layer1, avg_class.average(w2)) + avg_class.average(b2)\n",
    "#         return layer2\n",
    "\n",
    "def inference(input_tensor):\n",
    "    with tf.variable_scope(\"layer1\", reuse=tf.AUTO_REUSE):\n",
    "        weights = tf.get_variable(\"weights\", [INPUT_SIZE, HIDDEN_SIZE], \n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        biases = tf.get_variable(\"biases\", [HIDDEN_SIZE], \n",
    "                                  initializer=tf.constant_initializer(0.0))\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights) + biases)\n",
    "    \n",
    "    with tf.variable_scope(\"layer2\", reuse=tf.AUTO_REUSE):\n",
    "        weights = tf.get_variable(\"weights\", [HIDDEN_SIZE, OUTPUT_SIZE], \n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        biases = tf.get_variable(\"biases\", [OUTPUT_SIZE], \n",
    "                                  initializer=tf.constant_initializer(0.0))\n",
    "        layer2 = tf.matmul(layer1, weights) + biases\n",
    "    return layer2\n",
    "\n",
    "def train(mnist):\n",
    "    feature = tf.placeholder(tf.float32, [None, INPUT_SIZE], name=\"feature\")\n",
    "    label = tf.placeholder(tf.float32, [None, OUTPUT_SIZE], name=\"label\")\n",
    "#     w1 = tf.Variable(tf.truncated_normal([INPUT_SIZE, HIDDEN_SIZE], stddev=0.1))\n",
    "#     b1 = tf.Variable(tf.constant(0., shape=[HIDDEN_SIZE]))\n",
    "#     w2 = tf.Variable(tf.truncated_normal([HIDDEN_SIZE, OUTPUT_SIZE], stddev=0.1))\n",
    "#     b2 = tf.Variable(tf.constant(0., shape=[OUTPUT_SIZE]))\n",
    "    #     y = inference(feature, None, w1, b1, w2, b2)\n",
    "    y = inference(feature)\n",
    "    \n",
    "    # use moving averages\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "#     variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "#     variable_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "#     y_average = inference(feature, variable_averages, w1, b1, w2, b2) \n",
    "    \n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.argmax(label,1), logits=y)\n",
    "    reg = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    loss = tf.reduce_mean(cross_entropy) + reg(w1) + reg(w2)\n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE, global_step, mnist.train.num_examples/BATCH_SIZE, \n",
    "                                           LEARNING_RATE_DECAY)\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    train_op = tf.group([train_step, variable_averages_op])\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(label, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            data_x, data_y = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op, feed_dict={feature: data_x, label: data_y})\n",
    "        \n",
    "            if i%1000==0:\n",
    "                valid_acc = sess.run(accuracy, feed_dict={feature: mnist.validation.images,\n",
    "                                                      label: mnist.validation.labels})\n",
    "                test_acc = sess.run(accuracy, feed_dict={feature: mnist.test.images,\n",
    "                                                      label: mnist.test.labels})\n",
    "                print(\"Valid acc at step {}: {}\".format(i, valid_acc))\n",
    "                print(\"Test acc at step {}: {}\".format(i, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
      "Valid acc at step 0: 0.2937999963760376\n",
      "Test acc at step 0: 0.28029999136924744\n",
      "Valid acc at step 1000: 0.9757999777793884\n",
      "Test acc at step 1000: 0.9740999937057495\n",
      "Valid acc at step 2000: 0.9787999987602234\n",
      "Test acc at step 2000: 0.9778000116348267\n",
      "Valid acc at step 3000: 0.978600025177002\n",
      "Test acc at step 3000: 0.9782999753952026\n",
      "Valid acc at step 4000: 0.9836000204086304\n",
      "Test acc at step 4000: 0.98089998960495\n",
      "Valid acc at step 5000: 0.9828000068664551\n",
      "Test acc at step 5000: 0.9814000129699707\n",
      "Valid acc at step 6000: 0.9833999872207642\n",
      "Test acc at step 6000: 0.9825999736785889\n",
      "Valid acc at step 7000: 0.984000027179718\n",
      "Test acc at step 7000: 0.9818999767303467\n",
      "Valid acc at step 8000: 0.9833999872207642\n",
      "Test acc at step 8000: 0.9822999835014343\n",
      "Valid acc at step 9000: 0.9836000204086304\n",
      "Test acc at step 9000: 0.982200026512146\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"mnist_data/\",one_hot=True)\n",
    "train(mnist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
