{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "reader_path = \"../\" \n",
    "sys.path.append(os.path.abspath(reader_path))\n",
    "import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    # The PTB dataset comes from Tomas Mikolov's webpage:\n",
    "    # http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
    "    data_path = \"/tmp/ptb_data\"\n",
    "    init_scale = 0.05\n",
    "    hidden_size = 256\n",
    "    num_layers = 2\n",
    "    vocab_size = 10000\n",
    "    learning_rate = 1.0\n",
    "    batch_size = 16\n",
    "    num_steps = 32\n",
    "#     eval_batch_size = 1\n",
    "#     eval_num_steps = 1\n",
    "    num_epoch = 2\n",
    "    keep_prob = 0.5\n",
    "    max_grad_norm = 5\n",
    "\n",
    "def get_config():\n",
    "    return Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "    def __init__(self, is_train, config, data):\n",
    "        self.batch_size = config.batch_size\n",
    "        self.num_steps = config.num_steps\n",
    "        self.epoch_size = ((len(data) // self.batch_size) - 1) // self.num_steps\n",
    "        self.input_data, self.targets = reader.ptb_producer(\n",
    "            data, self.batch_size, self.num_steps)\n",
    "        vocab_size = config.vocab_size\n",
    "        hidden_size = config.hidden_size\n",
    "        \n",
    "        embedding = tf.get_variable(\"embedding\", [vocab_size, hidden_size])\n",
    "        inputs = tf.nn.embedding_lookup(embedding, self.input_data) # batch_size*num_steps*hidden_size\n",
    "        if is_train and config.keep_prob<1:\n",
    "            inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "        \n",
    "        output, state = self._build_lstm_graph(inputs, config, is_train)  \n",
    "        \n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [hidden_size, vocab_size])\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "        logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "        # Reshape logits to be a 3-D tensor for sequence loss\n",
    "        logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n",
    "\n",
    "        loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            logits,\n",
    "            self.targets,\n",
    "            tf.ones([self.batch_size, self.num_steps], dtype = tf.float32),\n",
    "            average_across_batch=True)\n",
    "        self.cost = tf.reduce_sum(loss)\n",
    "        self.final_state = state\n",
    "        \n",
    "        if not is_train:\n",
    "            return\n",
    "        \n",
    "        # 控制梯度膨胀\n",
    "        train_vars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(self.cost, train_vars), config.max_grad_norm)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(config.learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(\n",
    "            zip(grads, train_vars))\n",
    "    \n",
    "    \n",
    "    def _build_lstm_graph(self, inputs, config, is_train):\n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(config.hidden_size)\n",
    "        # Use deep RNN with dropout\n",
    "        if is_train and config.keep_prob < 1:\n",
    "            lstm_cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                lstm_cell, output_keep_prob=config.keep_prob)\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * config.num_layers)\n",
    "        self.initial_state = cell.zero_state(config.batch_size, tf.float32)\n",
    "        state = self.initial_state\n",
    "        outputs = []\n",
    "        with tf.variable_scope(\"RNN\", reuse=tf.AUTO_REUSE):\n",
    "            for time_step in range(self.num_steps):\n",
    "#                 if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "                output, state = cell(inputs[:, time_step, :], state)\n",
    "                outputs.append(output)        \n",
    "        output = tf.reshape(tf.concat(outputs, 1), [-1, config.hidden_size])\n",
    "        return output, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(sess, model, train_op=tf.no_op()):\n",
    "    tota_costs = 0.0\n",
    "    iters = 0\n",
    "    state = sess.run(model.initial_state)\n",
    "    for step in range(model.epoch_size):\n",
    "        print(step)\n",
    "        cost, state, _ = sess.run([model.cost, model.final_state, train_op],\n",
    "                                  feed_dict={model.initial_state: state})\n",
    "        print(cost)\n",
    "        total_costs += cost\n",
    "        iters += model.num_steps\n",
    "        \n",
    "        if step % (model.epoch_size // 10) == 0:\n",
    "            print(\"Perplexity at step %d is %.3f\" % (step, np.exp(total_costs/iters)))\n",
    "    return np.exp(total_costs/iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    config = get_config()\n",
    "    train_data, valid_data, test_data, _ = reader.ptb_raw_data(config.data_path)\n",
    "    initializer = tf.random_uniform_initializer(-config.init_scale,\n",
    "                                                config.init_scale)\n",
    "    with tf.name_scope(\"train\"):\n",
    "        with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "            train_model = PTBModel(is_train=True, config=config, data=train_data)\n",
    "        tf.summary.scalar(\"Training Loss\", train_model.cost)\n",
    "#         tf.summary.scalar(\"Learning Rate\", model.learning_rate)\n",
    "    \n",
    "    with tf.name_scope(\"validation\"):\n",
    "        with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n",
    "            valid_model = PTBModel(is_train=False, config=config, data=valid_data)\n",
    "        tf.summary.scalar(\"Validation Loss\", valid_model.cost)\n",
    "    \n",
    "    with tf.name_scope(\"test\"):\n",
    "        with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n",
    "            test_model = PTBModel(is_train=False, config=config, data=test_data)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(config.num_epoch):\n",
    "            train_perplexity = run_epoch(sess, train_model, train_model.train_op)\n",
    "            print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "#             valid_perplexity = run_epoch(sess, valid_model)\n",
    "#             print(\"Epoch: %d Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "#         test_perplexity = run_epoch(sess, test_model)\n",
    "#         print(\"Test Perplexity: %.3f\" % test_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0322 14:43:34.011439 140447397373696 deprecation.py:323] From /home/johnzyh/Jupyter/deep learning/lstm/reader.py:120: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "W0322 14:43:34.015135 140447397373696 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:320: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "W0322 14:43:34.015943 140447397373696 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "W0322 14:43:34.018809 140447397373696 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "W0322 14:43:34.021136 140447397373696 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "W0322 14:43:34.022836 140447397373696 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/input.py:202: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0322 14:43:34.069178 140447397373696 deprecation.py:506] From <ipython-input-3-83124d100dc5>:14: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0322 14:43:34.080376 140447397373696 deprecation.py:323] From <ipython-input-3-83124d100dc5>:45: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0322 14:43:34.084716 140447397373696 deprecation.py:323] From <ipython-input-3-83124d100dc5>:50: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "W0322 14:43:34.085684 140447397373696 rnn_cell_impl.py:1622] At least two cells provided to MultiRNNCell are the same object and will share weights.\n",
      "W0322 14:43:34.116213 140447397373696 deprecation.py:506] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:737: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0322 14:43:37.825005 140447397373696 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "I0322 14:43:37.856920 140447397373696 summary_op_util.py:92] Summary name Training Loss is illegal; using Training_Loss instead.\n",
      "I0322 14:43:38.739596 140447397373696 summary_op_util.py:92] Summary name Validation Loss is illegal; using Validation_Loss instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
