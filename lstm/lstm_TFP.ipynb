{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "ename": "DuplicateFlagError",
     "evalue": "The flag 'log_dir' is defined twice. First from absl.logging, Second from /home/johnzyh/.local/lib/python3.5/site-packages/ipykernel_launcher.py.  Description from first occurrence: directory to write logfiles into",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDuplicateFlagError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9d54bd736015>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m tf.flags.DEFINE_string(\"log_dir\",\n\u001b[1;32m     35\u001b[0m     \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/tmp/log\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     help=\"Directory to put training log\")\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mFLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[0;34m'Use of the keyword argument names (flag_name, default_value, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m           'docstring) is deprecated, please use (name, default, help) instead.')\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_string\u001b[0;34m(name, default, help, flag_values, **args)\u001b[0m\n\u001b[1;32m    239\u001b[0m   \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m   \u001b[0mserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m   \u001b[0mDEFINE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE\u001b[0;34m(parser, name, default, help, flag_values, serializer, module_name, **args)\u001b[0m\n\u001b[1;32m     80\u001b[0m   \"\"\"\n\u001b[1;32m     81\u001b[0m   DEFINE_flag(_flag.Flag(parser, serializer, name, default, help, **args),\n\u001b[0;32m---> 82\u001b[0;31m               flag_values, module_name)\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_flag\u001b[0;34m(flag, flag_values, module_name)\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;31m# Copying the reference to flag_values prevents pychecker warnings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m   \u001b[0mfv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m   \u001b[0mfv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m   \u001b[0;31m# Tell flag_values who's defining the flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, flag)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m# module is simply being imported a subsequent time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDuplicateFlagError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m     \u001b[0mshort_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;31m# If a new flag overrides an old one, we need to cleanup the old flag's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDuplicateFlagError\u001b[0m: The flag 'log_dir' is defined twice. First from absl.logging, Second from /home/johnzyh/.local/lib/python3.5/site-packages/ipykernel_launcher.py.  Description from first occurrence: directory to write logfiles into"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import os\n",
    "import functools\n",
    "from absl import flags\n",
    "from observations import text8\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "tf.flags.DEFINE_float(\"learning_rate\",\n",
    "                      default=5e-3,\n",
    "                      help=\"Initial learning rate.\")\n",
    "tf.flags.DEFINE_integer(\"n_epoch\",\n",
    "                        default=200,\n",
    "                        help=\"number of epochs.\")\n",
    "tf.flags.DEFINE_integer(\"batch_size\",\n",
    "                        default=128,\n",
    "                        help=\"Batch size.\")\n",
    "tf.flags.DEFINE_integer(\"hidden_size\",\n",
    "                        default=512,\n",
    "                        help=\"Hidden layer size.\")\n",
    "tf.flags.DEFINE_integer(\"timesteps\", default=64, help=\"\")\n",
    "flags.DEFINE_string(\n",
    "    \"model_dir\",\n",
    "    default=os.path.join(os.getenv(\"TEST_TMPDIR\", \"/tmp\"),\n",
    "                         \"lstm/\"),\n",
    "    help=\"Directory to put the model's fit.\")\n",
    "flags.DEFINE_string(\"data_dir\",\n",
    "    default=\"/tmp/data\",\n",
    "    help=\"Directory to store file or otherwise file will be downloaded and extracted there\")\n",
    "tf.flags.DEFINE_string(\"log_dir\",\n",
    "    default=\"/tmp/log\",\n",
    "    help=\"Directory to put training log\")\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(input, batch_size, timesteps, encoder):\n",
    "  \"\"\"Generate batch with respect to input (a list). Encode its\n",
    "  strings to integers, returning an array of shape [batch_size, timesteps].\n",
    "  \"\"\"\n",
    "  while True:\n",
    "    imb = np.random.randint(0, len(input) - timesteps, batch_size)\n",
    "    encoded = np.asarray(\n",
    "        [[encoder[c] for c in input[i:(i + timesteps)]] for i in imb],\n",
    "        dtype=np.int32)\n",
    "    yield encoded\n",
    "\n",
    "def build_input_pipeline(generator, x_train, batch_size, timesteps, encoder):\n",
    "  train_dataset = tf.data.Dataset.from_generator(\n",
    "      functools.partial(generator, x_train, batch_size, timesteps, encoder),\n",
    "      output_types= tf.int64,\n",
    "      output_shapes=(tf.TensorShape([batch_size, timesteps])))\n",
    "  \n",
    "  train_iterator = train_dataset.make_initializable_iterator()\n",
    "  x_ph = train_iterator.get_next()\n",
    "  return x_ph, train_iterator\n",
    "    \n",
    "def lstm_cell(x, h, c, name=None, reuse=False):\n",
    "  \"\"\"LSTM returning hidden state and content cell at a specific timestep.\"\"\"\n",
    "  nin = x.shape[-1].value\n",
    "  nout = h.shape[-1].value\n",
    "  with tf.variable_scope(name, default_name=\"lstm\",\n",
    "                         values=[x, h, c], reuse=reuse):\n",
    "    wx = tf.get_variable(\"kernel/input\", [nin, nout * 4],\n",
    "                         dtype=tf.float32,\n",
    "                         initializer=tf.orthogonal_initializer(1.0))\n",
    "    wh = tf.get_variable(\"kernel/hidden\", [nout, nout * 4],\n",
    "                         dtype=tf.float32,\n",
    "                         initializer=tf.orthogonal_initializer(1.0))\n",
    "    b = tf.get_variable(\"bias\", [nout * 4],\n",
    "                        dtype=tf.float32,\n",
    "                        initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "  z = tf.matmul(x, wx) + tf.matmul(h, wh) + b\n",
    "  i, f, o, u = tf.split(z, 4, axis=1)\n",
    "  i = tf.sigmoid(i)\n",
    "  f = tf.sigmoid(f + 1.0)\n",
    "  o = tf.sigmoid(o)\n",
    "  u = tf.tanh(u)\n",
    "  c = f * c + i * u\n",
    "  h = o * tf.tanh(c)\n",
    "  return h, c\n",
    "\n",
    "def language_model(input, vocab_size):\n",
    "  \"\"\"Form p(x[0], ..., x[timesteps - 1]),\n",
    "\n",
    "  \\prod_{t=0}^{timesteps - 1} p(x[t] | x[:t]),\n",
    "\n",
    "  To calculate the probability, we call log_prob on\n",
    "  x = [x[0], ..., x[timesteps - 1]] given\n",
    "  `input` = [0, x[0], ..., x[timesteps - 2]].\n",
    "\n",
    "  We implement this separately from the generative model so the\n",
    "  forward pass, e.g., embedding/dense layers, can be parallelized.\n",
    "\n",
    "  [batch_size, timesteps] -> [batch_size, timesteps]\n",
    "  \"\"\"\n",
    "  x = tf.one_hot(input, depth=vocab_size, dtype=tf.float32) #(128,64,27)\n",
    "  h = tf.zeros([FLAGS.batch_size, FLAGS.hidden_size]) #(128,512)\n",
    "  c = tf.zeros([FLAGS.batch_size, FLAGS.hidden_size])\n",
    "  hs = []\n",
    "  reuse = None\n",
    "\n",
    "  for t in range(FLAGS.timesteps):\n",
    "    if t > 0:\n",
    "      reuse = True\n",
    "    xt = x[:, t, :]\n",
    "    h, c = lstm_cell(xt, h, c, name=\"lstm\", reuse=reuse) #(128,512)\n",
    "    hs.append(h)\n",
    "\n",
    "  h = tf.stack(hs, axis=1) #(128,64,512)\n",
    "  logits = tf.layers.dense(h, vocab_size, name=\"dense\") #(128,64,27)\n",
    "  output = tfd.Categorical(logits=logits)\n",
    "  return output\n",
    "\n",
    "def language_model_gen(batch_size, vocab_size):\n",
    "    \"\"\"\n",
    "    Generates x ~ prod p(x_t | x_{<t}). Output [batch_size, timesteps].\n",
    "    \"\"\"\n",
    "    # Initialize data input randomly.\n",
    "    x = tf.random_uniform([batch_size], 0, vocab_size, dtype=tf.int32)\n",
    "    h = tf.zeros([batch_size, FLAGS.hidden_size])\n",
    "    c = tf.zeros([batch_size, FLAGS.hidden_size])\n",
    "    xs = []\n",
    "    for _ in range(FLAGS.timesteps):\n",
    "        x = tf.one_hot(x, depth=vocab_size, dtype=tf.float32) #(5,27)\n",
    "        h, c = lstm_cell(x, h, c, name=\"lstm\") #(5,512)\n",
    "        logits = tf.layers.dense(h, vocab_size, name=\"dense\") #(5,27)\n",
    "        x = tfd.Categorical(logits=logits).sample() #(5,)\n",
    "        xs.append(x)\n",
    "\n",
    "    xs = tf.cast(tf.stack(xs, 1), tf.int32) #(5,64)\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "\n",
    "# DATA\n",
    "  x_train, _, x_test = text8(FLAGS.data_dir)\n",
    "  x_train = x_train[:1000000]\n",
    "  x_test = x_test[:50000]\n",
    "  vocab = string.ascii_lowercase + ' '\n",
    "  vocab_size = len(vocab)\n",
    "  encoder = dict(zip(vocab, range(vocab_size)))\n",
    "  decoder = {v: k for k, v in encoder.items()}\n",
    "\n",
    "  data = generator(x_train, FLAGS.batch_size, FLAGS.timesteps, encoder)\n",
    "\n",
    "# MODEL\n",
    "  x_ph, train_iterator = build_input_pipeline(generator, x_train, FLAGS.batch_size, \n",
    "                                              FLAGS.timesteps, encoder)\n",
    "  with tf.variable_scope(\"language_model\", reuse=tf.AUTO_REUSE):\n",
    "  # Shift input sequence to right by 1, [0, x[0], ..., x[timesteps - 2]].\n",
    "    x_ph_shift = tf.pad(x_ph, [[0, 0], [1, 0]])[:, :-1]\n",
    "    x = language_model(x_ph_shift, vocab_size)\n",
    "    x_gen = language_model_gen(5, vocab_size)\n",
    "  \n",
    "  imb = range(0, len(x_test) - FLAGS.timesteps, FLAGS.timesteps)\n",
    "  encoded_x_test = np.asarray(\n",
    "      [[encoder[c] for c in x_test[i:(i + FLAGS.timesteps)]] for i in imb],\n",
    "      dtype=np.int32)\n",
    "  test_size = encoded_x_test.shape[0]\n",
    "  print(\"Test set shape: {}\".format(encoded_x_test.shape))\n",
    "  test_nll = -tf.reduce_sum(x.log_prob(x_ph))\n",
    "\n",
    "  train_nll = -tf.reduce_sum(x.log_prob(x_ph))\n",
    "  optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "  train_op = optimizer.minimize(train_nll)\n",
    "\n",
    "  init_op = tf.group(tf.global_variables_initializer(),\n",
    "                     tf.local_variables_initializer())\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    sess.run(train_iterator.initializer)\n",
    "    \n",
    "    # Double n_epoch and print progress every half an epoch.\n",
    "    n_iter_per_epoch = len(x_train) // (FLAGS.batch_size * FLAGS.timesteps * 2)\n",
    "    epoch = 0.0\n",
    "    for _ in range(2):\n",
    "      epoch += 0.5\n",
    "      print(\"Epoch: {0}\".format(epoch))\n",
    "      avg_nll = 0.0\n",
    "\n",
    "      for t in range(1, n_iter_per_epoch + 1):\n",
    "        [_, train_nll_] = sess.run([train_op, train_nll])\n",
    "        avg_nll += train_nll_\n",
    "\n",
    "      # Print average bits per character over epoch.\n",
    "      avg_nll /= (n_iter_per_epoch * FLAGS.batch_size * FLAGS.timesteps *\n",
    "                np.log(2))\n",
    "      print(\"Train average bits/char: {:0.8f}\".format(avg_nll))\n",
    "\n",
    "      # Print per-data point log-likelihood on test set.\n",
    "      avg_nll = 0.0\n",
    "      for start in range(0, test_size, FLAGS.batch_size):\n",
    "        end = min(test_size, start + FLAGS.batch_size)\n",
    "        x_batch = encoded_x_test[start:end]\n",
    "        avg_nll += sess.run(test_nll, {x_ph: x_batch})\n",
    "\n",
    "      avg_nll /= test_size\n",
    "      print(\"Test average NLL: {:0.8f}\".format(avg_nll))\n",
    "      \n",
    "    # Generate samples from model.\n",
    "      samples = sess.run(x_gen)\n",
    "      samples = [''.join([decoder[c] for c in sample]) for sample in samples]\n",
    "      print(\"Samples:\")\n",
    "      for sample in samples:\n",
    "        print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
